---
layout: post
title: "Asymmetric Learning for Privacy-Preserving ML"
categories: research
github_comments_issueid: "1"
use_math: true
author:
- Yue (Julien) Niu
---

**Training and inference costs** have always been the central problem 
when deploy machine learning (ML) systems. 
This issue is even more urgent when ML meets **privacy**.
One privacy-preserving ML method, based on **trusted execution environments (TEEs)**, 
stores data and perform all computation inside TEEs. 

However, current TEE environments are usually supported in CPU platforms, 
making them much less computationally efficient compared to GPU platforms. 
**Therefore, it is very needed to explore a new training and inference workflow with TEEs that
provides strong privacy protection and also achieves high computing performance.**

## Overview

In this research project, we are targeting the problem above by 
**exploiting input low-rank structure and its potentials in reducing computation costs**.
Low-rank structure is widely exploited in models, but 
is rarely investigated in input or internal activations.
For convolutional neural networks (CNNs), 
input $X \in R^{c\times h \times w}$ usually present high correlation among channels. 