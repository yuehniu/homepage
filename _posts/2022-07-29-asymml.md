---
layout: post
title: "Asymmetric Learning for Privacy-Preserving ML"
categories: research
github_comments_issueid: "1"
use_math: true
author:
- Yue (Julien) Niu
---

**Training and inference costs** have always been the central problem 
when deploy machine learning (ML) systems. 
This issue is even more urgent when ML meets **privacy**.
One privacy-preserving ML method, based on **trusted execution environments (TEEs)**, 
stores data and perform all computation inside TEEs. 

However, current TEE environments are usually supported in CPU platforms, 
making them much less computationally efficient compared to GPU platforms. 
**Therefore, it is very needed to explore a new training and inference workflow with TEEs that
provides strong privacy protection and also achieves high computing performance.**

## Overview

In this research project, we are targeting the problem above by 
**exploiting input low-rank structure and its potentials in reducing computation costs**.
Low-rank structure is widely exploited in models, but 
is rarely investigated in input or internal activations.
For convolutional neural networks (CNNs), 
input $X \in R^{c\times h \times w}$ usually present high correlation among $c$ channels. 
As a result, we can leverage the property and approximate the input with 
much fewer channels ($r \ll c$).
Unlike low-rank structure in model parameters, the low-rank structure in inputs play a key role
in efficient privacy-preserving machine learning via an asymmetric learning.

>From a high-level view, we approximate model's input with a low-dimensional presentation, and
perform computation with the low-dimensional input in TEEs.
> On the other hand, we do not discard the residual information. 
> Instead, we offload the residual information onto GPUs and let GPUs accelerate computation
> on the residual part. See the figure below:


<img src="https://yuehniu.github.io/homepage//assets/fig/asymml/overview.png" alt="Overview of the Asymmetric Learning" width="600"/>

Suppose