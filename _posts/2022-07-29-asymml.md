---
layout: post
title: "Asymmetric Learning for Privacy-Preserving ML"
categories: research
github_comments_issueid: "1"
use_math: true
author:
- Yue (Julien) Niu
---

**Training and inference costs** have always been the central problem 
when deploy machine learning (ML) systems. 
This issue is even more urgent when ML meets **privacy**.
One privacy-preserving ML method, based on **trusted execution environments (TEEs)**, 
stores data and perform all computation inside TEEs. 

However, current TEE environments are usually supported in CPU platforms, 
making them much less computationally efficient compared to GPU platforms. 
**Therefore, it is very needed to explore a new training and inference workflow with TEEs that
provides strong privacy protection and also achieves high computing performance.**

## Overview

In this research project, we are targeting the problem above by 
**exploiting input low-rank structure and its potentials in reducing computation costs**.
Low-rank structure is widely exploited in models, but 
is rarely investigated in input or internal activations.
For convolutional neural networks (CNNs), 
input $X \in R^{c\times h \times w}$ usually present high correlation among $c$ channels. 
As a result, we can leverage the property and approximate the input with 
much fewer channels ($r \ll c$).
Unlike low-rank structure in model parameters, the low-rank structure in inputs play a key role
in efficient privacy-preserving machine learning via an asymmetric learning.

>From a high-level view, we approximate model's input with a low-dimensional presentation, and
perform computation with the low-dimensional input in TEEs.
> On the other hand, we do not discard the residual information. 
> Instead, we offload the residual information onto GPUs and let GPUs accelerate computation
> on the residual part. See the figure below:


<img src="https://yuehniu.github.io/homepage//assets/fig/asymml/overview.png" alt="Overview of the Asymmetric Learning" width="600"/>

### Data Decomposition
A central problem is to find a way to effectively decompose input data and 
encode information-sensitive part into low-dimensional space.

We represent input as $X \in R^{c \times hw}$, where $c$ denotes the number of channels, 
$h,w$ is height and width of each image. Then we apply SVD on X as:

\[ X \rightarrow \sum_{i=1}^c \sigma_i \cdot u_i \cdot X'_i \]

where $X'_i$ denotes $i$-th principal channel, and 
$\sigma_i$ denotes the importance of the channel.

With the notation of channels' importance, we obtain a low-rank approximation
with the first $r$ most principal channel as

\[ X^T = \sum_{i=1}^r \sigma_i \cdot u_i \cdot X'_i \]

While we still keep the residual: $X^U = X - X^T$.

Due to input's low-rank property, the residual can be safely outsourced to GPUs 
without leaking significant privacy.

```
Note: 
The Exact SVD incurs great computational costs. 
In the implementation, we adopt a lighweight SVD approximation 
without affecting SVD performance. 
```