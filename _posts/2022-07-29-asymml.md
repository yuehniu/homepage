---
layout: post
title: "Private Learning with Asymmetric Flows"
categories: research
github_comments_issueid: "1"
use_math: true
author:
- Yue (Julien) Niu
- Ramy E. Ali
- Saurav Prakash
- Salman Avestimehr
---

<p style="text-align: center; font-size: 14pt">
<a href="https://arxiv.org/abs/2312.05264" class="btn">Paper</a>
<a href="https://github.com/yuehniu/asymmetricML" class="btn">Code</a>
</p>

**Training and inference costs** have always been the central problem 
when deploy machine learning (ML) systems. 
This issue is even more urgent when ML meets **privacy**.

In this project, we explore a new private learning setting, where there are a public platform (e.g. cloud GPUs) and 
a private platform (e.g., local environment). We want to develop a new learning and inference framework that is aimed
to achieve efficient training while still maintain strong privacy protection.

---

## Overview

We first use two asymmetric data decomposition: singular value decomposition (SVD) and discrete cosine transform (DCT)
to extract the low-dimensional main representation of input and the residual part.

We design two seperated model to learning the low-dimensional representation and the residual part. The model that 
learns low-dimensional representation run in private environment; while the model that learns residuals runs in the 
public environment. Therefore, both efficiency and privacy is achieved. 

<p style="text-align: center;">
<img src="https://yuehniu.github.io/homepage//assets/fig/asymml/overview.png" alt="Overview of the Asymmetric Learning" width="700"/>
</p>

---

### Data Decomposition
A central problem is to find a way to effectively decompose input data and 
encode information-sensitive part into low-dimensional space.

We represent input as $X \in R^{c \times hw}$, where $c$ denotes the number of channels, 
$h,w$ is height and width of each image. Then we apply SVD on X as:

\[ X \rightarrow \sum_{i=1}^c \sigma_i \cdot u_i \cdot X'_i \]

where $X'_i$ denotes $i$-th principal channel, and 
$\sigma_i$ denotes the importance of the channel.

With the notation of channels' importance, we obtain a low-rank approximation
with the first $r$ most principal channel as

\[ X^T = \sum_{i=1}^r \sigma_i \cdot u_i \cdot X'_i \]

While we still keep the residual: $X^U = X - X^T$.

Due to input's low-rank property, the residual can be safely outsourced to GPUs 
without leaking significant privacy.

>The Exact SVD incurs great computational costs. 
>In the implementation, we adopt a lighweight SVD approximation without affecting SVD performance.

---

### Model Decomposition

With input decomposed, 
the model also needs to be adapted to split the workloads in forward and backward passes.

For a convolution layer with kernel $W$, we can easily reduce the convolution on $X^T$ as

\[ Y = \sum_{i=1}^r X' \circledast W' \]

where $W'$ is the transformed kernel given low-rank coefficient $u$ 
(If you are interested, please detail in the paper).

> Therefore, the computation complexity in TEEs is reduced to $O(r)$.
> With $r \ll c$, running time in TEEs can be significantly shortened. 

With the low-rank input, backward passes can be also reduced with complexity $O(r)$.
In the paper, we present a full procedure for the decomposition on the forward and backward passes.
