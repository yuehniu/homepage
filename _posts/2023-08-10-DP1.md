---
layout: post
title: "Differential Privacy: Basic Concepts"
categories: research
github_comments_issueid: "3"
use_math: true
author:
- Yue (Julien) Niu
---

Differential Privacy (DP) provides a formal privacy framework that 
helps analyze privacy leakage for a system.
However, as a beginner in privacy-related fields, we found it difficult to understand 
even basic concepts in DP. 
For instance, 

- how to understand neighboring datasets?
- How to understand the DP definition itself?

It is, however, crucial to have a clear understanding of DP 
before developing or adapting our own privacy protection solutions.

This blog provides a thorough but easy-to-follow description of the basic concepts of DP 
and standard DP mechanisms. 
Importantly, how to theoretically analyze privacy leakage for different mechanisms?

---

## Understanding Basic DP concepts

### A General Description of DP

<img src="https://yuehniu.github.io/homepage//assets/fig/dp/dp_overview.png" alt="A general description of DP" width="800"/>


Given a system with input $X$, as shown in Figure above, a DP mechanism introduces 
certain randomness to the system's output, so the input-output mapping is no longer 
deterministic. As a result, the outputs are non-deterministic regardless of what the inputs are.

The essence of differential privacy (DP) is to provide a metric that measures 
*how possible an attacker can tell which dataset a data record comes from*, 
as shown in Figure above.

It does not reflect how much information about $X$ is leaked from a system's output, 
which might be why many beginners with information background find DP not intuitive. 

A typical question may arise: 
- Will it be easier to define in a way that measures information leakage about $X$ 
from the system's output?

    However, such a definition is conceptually intuitive (in terms of information), 
    but not realistic in practice. 
    The reason is that it is impossible to know the distribution of $X$ and 
    the system's output in real problems. We cannot provide a formal privacy analysis 
    if we do not know the distributions.
    DP is not intuitive at first glance but is more viable in terms of theoretical analysis.

- Is it good enough to measure privacy leakage using DP?

    The answer to the question is not straightforward. 
    First, it seems not a big deal if an attacker knows if a single data record comes from 
    the target dataset, as it does not indicate the attacker knows every record of 
    the whole target dataset.
    However, assuming the attacker has many ways to pull a large number of samples 
    from the Internet or other public sources, 
    it can feed every collected record to the system and infer 
    if some record exists in the target dataset.
    As a result, the attacker may retrieve quite a few data records from the target dataset 
    if the target dataset also has some similar records.

### Basic Concepts: Neighboring Datasets and Sensitivity

The concept of neighboring datasets is another challenge that makes DP hard to digest. 
Essentially, DP aims to guarantee that an attacker cannot differentiate between two *close* datasets. 
However, it is a little subtle to quantify how close the datasets are.
If we look into dataset $D = \left \{ D^1, \cdots, D^i, \cdots, D^N \right \}$ and 
$D' = \left \{ D'^1, \cdots, D'^i, \cdots, D'^N \right \}$, 
each dataset has multiple records, and each record can represent images, tables or other attributes.
It is not straightforward to measure the *closeness* of $D$ and $D'$.

The way that DP deal with the *closeness* is to define so-called *neighboring datasets* 
$D$ and $D'$ such that $D$ and $D'$ are almost identical except one record 
(e.g., $D^i$ and $D'^i$) is different.
This definition is also where the *differential* comes from.

But it is not good yet with only the *neighboring datasets*. 
We still need to formally quantify how different $D$ and $D'$ are.
Given only $D^i$ and $D'^i$ are different, it becomes easier to measure the difference 
as we can use a certain *distance measure* to compute the difference, so-called *sensitivity* in DP.
Sensitivity can be defined in multiple ways, depending on what DP mechanism is used, 
which will be covered in later sections.