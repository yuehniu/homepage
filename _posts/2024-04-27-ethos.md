---
layout: post
title: "Private Learning with Asymmetric Flows"
categories: research
use_math: true
author:
- Yue (Julien) Niu
- Lei Gao
- Tingting Tang
- Salman Avestimehr
- Murali Annavaram
---

Language models (LMs) have greatly propelled the research on natural language processing. 
However, LMs also raise concerns regarding the generation of biased or toxic content and the potential disclosure of private information from the training dataset. 

In this project, we explore a new way to align and rectify language models to remove undesired knowledge.
We investigate popular language models such as GPT, Falcon. We show that these pre-trained language models can be edited
so that the models give less undesired responses. 

<a href="https://arxiv.org/abs/2403.08994" class="btn">Paper</a>
<a href="https://github.com/leigao97/ethos" class="btn">Code</a>

---

## Overview

For a pre-trained (language) model, model weights encode knowledge about the training dataset. 
That is, if a training dataset has biased records, this biased knowledge will be learned and encoded in weights after pre-training. 

Therefore, in this project, we aim to find a way to locate which parts of the model may encode the undesired knowledge. 
If we can locate them, then we can remove them from the pre-trained model. 

<p style="text-align: center;">
<img src="https://yuehniu.github.io/homepage//assets/fig/ethos/ethos.png" alt="Overview of the Asymmetric Learning" width="500"/>
</p>


---

### Citation

```latex
@misc{NAACL24Ethos,
      title={Ethos: Rectifying Language Models in Orthogonal Parameter Space}, 
      author={Lei Gao, Yue Niu, Tingting Tang, Salman Avestimehr, Murali Annavaram},
      year={2024},
      eprint={2403.08994},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

---

