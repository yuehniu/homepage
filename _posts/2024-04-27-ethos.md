---
layout: post
title: "Private Learning with Asymmetric Flows"
categories: research
use_math: true
author:
- Yue (Julien) Niu
- Lei Gao
- Tingting Tang
- Salman Avestimehr
- Murali Annavaram
---

Language models (LMs) have greatly propelled the research on natural language processing. 
However, LMs also raise concerns regarding the generation of biased or toxic content and the potential disclosure of private information from the training dataset. 

In this project, we explore a new way to align and rectify language models to remove undesired knowledge.
We investigate popular language models such as GPT, Falcon. We show that these pre-trained language models can be edited
so that the models give less undesired responses. 

<a href="https://arxiv.org/abs/2403.08994" class="btn">Paper</a>
<a href="https://github.com/leigao97/ethos" class="btn">Code</a>

---

## Overview

For a pre-trained (language) model, model weights encode knowledge about the training dataset. 
That is, if a training dataset has biased records, this biased knowledge will be learned and encoded in weights after pre-training. 

Therefore, in this project, we aim to find a way to locate which parts of the model may encode the undesired knowledge. 
If we can locate them, then we can remove them from the pre-trained model. 

<p style="text-align: center;">
<img src="https://yuehniu.github.io/homepage//assets/fig/ethos/ethos.png" alt="Overview of the Asymmetric Learning" width="500"/>
</p>

## Weight Analyzing in Orthogonal Space

Demystifying contribution of weights is non-trivial. Weights in pre-trained models are usually correlated, it is difficult to 
separate them and analyze their individual contribution to models' behavior. To address this challenge, we first convert weights 
of a pre-trained model in orthogonal space as

\[ W = \sum_{k=1} W_k = \sum_{k=1} s_k \cdot \boldsymbol{u_k} \cdot \boldmath{v_k^*} \]

where $s_k, \boldmath{u_k}, \boldmath{v_k}$ denotes singular values and principal vectors obtained from singular value decomposition (SVD). 

The reason for converting weights into orthogonal space is that we can effectively decompose weights and analyze contribution of each component.
In particular, given two orthogonal weight matrices $W_1, W_2$, we can easily see that their output given the same input $\boldmath{x}$ are also orthogonal:

\[ \boldmath{y}_1 = W_1 \cdot \boldmath{x}, \boldmath{y}_2 = W_2 \cdot \boldmath{x} \]


---

### Citation

```latex
@misc{NAACL24Ethos,
      title={Ethos: Rectifying Language Models in Orthogonal Parameter Space}, 
      author={Lei Gao, Yue Niu, Tingting Tang, Salman Avestimehr, Murali Annavaram},
      year={2024},
      eprint={2403.08994},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

---

