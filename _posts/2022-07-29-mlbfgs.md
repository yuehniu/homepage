---
layout: post
title: "High-Order Optimization on Large-Scale NN Training"
categories: research
github_comments_issueid: "2"
use_math: true
author:
- Yue (Julien) Niu
---

Second-order or quasi-Newton methods have been regard impractical for large-scale problems, such as
large neural net training.
The main reason stem from attaining the Hessian and its inverse, with quadratic complexity 
in terms of a problem size. 
Specifically, for a neural net with $d$ parameters to be optimized, obtaining the Hessian and
its inverse incurs $O(d^2)$ computation and memory complexity. 
Considering current deep learning models with millions of parameters, it seems impossible 
to apply high-order methods during optimization.

Along this line of research, we aim to design a second-order optimizer with linear complexity, 
and convey the promises of second-order methods to real DNN optimization.

[`Paper Link`](https://arxiv.org/abs/2307.13744) -- 
[`OpenReview`](https://openreview.net/forum?id=9jnsPp8DP3&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DTMLR%2FAuthors%23your-submissions)) ---
`Code will be out soon`