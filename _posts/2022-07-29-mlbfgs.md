---
layout: post
title: "High-Order Optimization on Large-Scale NN Training"
categories: research
github_comments_issueid: "2"
use_math: true
author:
- Yue (Julien) Niu
---

Second-order or quasi-Newton methods have been regard impractical for large-scale problems, such as
large neural net training.
The main reason stem from attaining the Hessian and its inverse, with quadratic complexity 
in terms of a problem size. 
Specifically, for a neural net with $d$ parameters to be optimized, obtaining the Hessian and
its inverse incurs $O(d^2)$ computation and memory complexity. 
Considering current deep learning models with millions of parameters, it seems impossible 
to apply high-order methods during optimization.

Along this line of research, **we aim to design a second-order optimizer with linear complexity, 
and convey the promises of second-order methods to real DNN optimization**.

[`Paper Link`](https://arxiv.org/abs/2307.13744) -- 
[`OpenReview`](https://openreview.net/forum?id=9jnsPp8DP3&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DTMLR%2FAuthors%23your-submissions)) ---
`Code will be out soon`

---

### Overview
In this project, we delve into one widely known quasi-Newton method called L-BFGS, and 
convey it to current large-scale NN optimization. 
L-BFGS, though can converge in a super-linear rate with proper step searching, 
it faces severe convergence instability issues in stochastic settings. 
Current works that adapt L-BFGS in stochastic learning usually incurs significant additional costs,
which goes against the objective of reducing complexity of quasi-Newton methods.

In this project, we propose a new **momentum**-based mechanism to stabilize L-BFGS in 
stochastic settings.
Like the momentum in SGD, the momentum-based L-BFGS in almost cost-free but still capture
second-order information to speed up the convergence.
We evaluate momentum-based L-BFGS on common benchmark datasets (including ImageNet) and models. 
Experiments shows momentum-based L-BFGS enjoys much faster wall-clock convergence rate compared 
to other quasi-Newton methods.

---

### Second-order Optimization
In a typical optimization problem, the objective function is usually written as 

\[ L(\theta;x) = \frac{1}{N} \sum_{i=1}^N l(\theta; x^i) \]

where $\theta$ are the parameters to be optimized, $x$ denotes the dataset. 

With the Hessian or its approximation, we usually have the update as 

\[ \theta_{t+1} = \theta_t - \eta \cdot H \cdot g_t \]

where $H$ denotes the Hessian inverse, $g_t$ denotes the gradients. 

With good estimate of the Hessian, the above optimization can be much faster than SGD.
However, computing the Hessian and its inverse have been considered impractical 
for large-scale model training.

### Where to Add Momentum

### A Toy Example

### Evaluations on Real NNs

### More Analysis