---
layout: post
title: "High-Order Optimization on Large-Scale NN Training"
categories: research
github_comments_issueid: "2"
use_math: true
author:
- Yue (Julien) Niu
---

Second-order or quasi-Newton methods have been regard impractical for large-scale problems, such as
large neural net training.
The main reason stem from attaining the Hessian and its inverse, with quadratic complexity 
in terms of a problem size. 
Specifically, for a neural net with $d$ parameters to be optimized, obtaining the Hessian and
its inverse incurs $O(d^2)$ computation and memory complexity. 
Considering current deep learning models with millions of parameters, it seems impossible 
to apply high-order methods during optimization.

Along this line of research, **we aim to design a second-order optimizer with linear complexity, 
and convey the promises of second-order methods to real DNN optimization**.

[`Paper Link`](https://arxiv.org/abs/2307.13744) -- 
[`OpenReview`](https://openreview.net/forum?id=9jnsPp8DP3&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DTMLR%2FAuthors%23your-submissions)) ---
`Code will be out soon`

---

### Overview
In this project, we delve into one widely known quasi-Newton method called L-BFGS, and 
convey it to current large-scale NN optimization. 
L-BFGS, though can converge in a super-linear rate with proper step searching, 
it faces severe convergence instability issues in stochastic settings. 
Current works that adapt L-BFGS in stochastic learning usually incurs significant additional costs,
which goes against the objective of reducing complexity of quasi-Newton methods.

In this project, we propose a new **momentum**-based mechanism to stabilize L-BFGS in 
stochastic settings.
Like the momentum in SGD, the momentum-based L-BFGS in almost cost-free but still capture
second-order information to speed up the convergence.
We evaluate momentum-based L-BFGS on common benchmark datasets (including ImageNet) and models. 
Experiments shows momentum-based L-BFGS enjoys much faster wall-clock convergence rate compared 
to other quasi-Newton methods.

---

### Where to Add Momentum

### A Toy Example

### Evaluations on Real NNs

### More Analysis