---
layout: home
---

**PhD candidate** in Department of ECE 

at **University of Southern California (USC)**.

<img src="assets/fig/yellowstone.jpeg" alt="drawing" width="100"/>
<img src="assets/fig/usc.png" alt="drawing" width="104"/>
<img src="assets/fig/viterbi.png" alt="drawing" width="105"/>

[`Scholar`](https://scholar.google.com/citations?user=J7vQ-QEAAAAJ&hl=en) ---
[`Linkedin`](https://www.linkedin.com/in/yue-niu-a3084216a/) ---
[`Github`](https://github.com/yuehniu) ---
[`CV`](https://drive.google.com/file/d/1kSlH8tRNY-JdNR_AdII9WraQwunO0BwR/view?usp=sharing)

## Bio  
  Yue (Julien) Niu is a Ph.D. candidate at the University of Southern California (USC), 
  working with Professor [Salman Avestimehr](https://www.avestimehr.com/), 
  the director of [Information Theory and Machine Learning (vITAL) Lab](https://www.avestimehr.com/vital-lab). 
  He also once worked with Professor [Viktor Prasanna](https://sites.usc.edu/prasanna/) 
  on [FPGA/Parallel Computing Lab](https://fpga.usc.edu/). 
  Before that, He got master’s and bachelor’s degree from Northwestern Polytechnical University in Xi’an, China.
  
  His research interests cover various aspects of `privacy in machine learning`, 
  including applying information theory and differential privacy 
  to enhance privacy protection in machine learning, federated learning, and 
  combining with TEEs to boost privacy protection in practice.  
  He also did research on `neural network acceleration` at the edge and `efficient machine learning optimization`.

---
  
## Research Highlight

  - **Efficient Private Machine Learning**

    - Privacy, bias, and fairness in language models
    
    - Differentially private machine learning with improved model utility

    - Private machine learning empowered by trusted execution environment

  - **CNN/Transformer/LLM Acceleration**

    - Fast training and inference via low-rank models and low-rank activation
    
    - Memory-efficient training and inference via low-rank and sparse compression
    
    - Accelerate neural networks with dedicated hardware

  - **Federated Learning at the Edge**
  
    - Federated learning of large models at resource-constrained devices
    
    - Communication-efficient federated learning with sparse training on clients

  - **Efficient High-Order Stochastic Optimization**
    
    - Distributed large-scale model training with quasi-Newton optimization

---

## News

--- `Jan 2024` ---  

<img src="assets/fig/new.png" alt="drawing" width="40"/>
Our paper, "Edge Private Graph Neural Networks with Singular Value Perturbation",
is accepted Privacy Enhancing Technologies Symposium (PETs), 2024.

--- `Dec 2023` ---

<img src="assets/fig/new.png" alt="drawing" width="40"/>
Our paper, "Ethos: Rectifying Language Models in Orthogonal Parameter Space",
is accepted as a spotlight presentation to AAAI Workshop on Responsible Language Models, 2023.

--- `Sep 2023` ---

<img src="assets/fig/new.png" alt="drawing" width="40"/>
Our paper, "Overcoming Resource Constraints in Federated Learning: Large Models Can Be Trained with only Weak Clients",
is accepted to Transaction on Machine Learning Research (TMLR), 2023.

<img src="assets/fig/new.png" alt="drawing" width="40"/>
I am invited as a member of the Program Committee (PC) of the 2024 [SIAM International Conference on Data Mining](https://www.siam.org/conferences/cm/conference/sdm24) (SDM’24), 
to be held in Houston, Texas, on April 18 to April 20, 2024.

<img src="assets/fig/new.png" alt="drawing" width="40"/>
Our paper, "Revisiting Sparsity Hunting in Federated Learning: Why the Sparsity Consensus Matters?",
is accepted to Transaction on Machine Learning Research (TMLR), 2023.

--- `August 2023` ---

<img src="assets/fig/new.png" alt="drawing" width="40"/>
I am chosen as a Viterbi Graduate Mentor to provide guidance to USC Viterbi new graduate students.

--- `July 2023` ---

<img src="assets/fig/new.png" alt="drawing" width="40"/>
Our paper, "mL-BFGS: A Momentum-based L-BFGS for Distributed Large-scale Neural Network Optimization",
is accepted to Transaction on Machine Learning Research (TMLR), 2023.

<img src="assets/fig/new.png" alt="drawing" width="40"/>
Our paper, "Performance and Failure Cause Estimation for Machine Learning Systems in the Wild", 
is accepted to International Conference on Computer Vision Systems (ICVS), 2023

---

## Experience

**Applied Scientist Intern** at *Amazon Alexa* <img src="assets/fig/alexa.png" alt="drawing" width="50"/>
`2022/6` --- `2022/9`  
`Topic`: Estimate a CV model’s performance in the wild  
`Collaborator`: Furqan Khan, Pradeep Natarajan, Ruoxi Liu

**Applied Scientist Intern** at *Amazon Alexa* <img src="assets/fig/alexa.png" alt="drawing" width="50"/>
`2021/6` --- `2021/9`  
`Topic`: Personalized model compression using knowledge distillation  
`Collaborator`: Furqan Khan, Pradeep Natarajan, Salman Avestimehr

**Research Intern** at *Tsinghua University* <img src="assets/fig/tsinghua.png" alt="drawing" width="40"/>
`2017/6` --- `2018/6`  
`Topic`: Neural network acceleration on FPGA  
`Collaborator`: Zhenyu Liu, Xiangyang Ji  
[`Demo`](https://youtu.be/eFW8OTIur38) --- [`Paper`](https://ieeexplore.ieee.org/abstract/document/8309067)

---

## Academic Service

**Conference Reviewer**: 

`ICLR` --- 2022(2) --- 2021(2)

`NeurIPS` --- 2023(6) --- 2022(4)

`ICML` --- 2023(4)    `KDD` --- 2023 (3)    `SDM` --- 2024 (3)

**Journal Reviewer**:

`Transactions on Mobile Computing` --- 2023 (1)

---

## Awards

**Best Poster Award** at [USC-Amazon Annual Symposium on Secure and Trusted ML](https://trustedai.usc.edu/)  
Los Angeles, 2023.